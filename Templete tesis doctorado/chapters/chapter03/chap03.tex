%
% File: chap02.tex
%
\let\textcircled=\pgftextcircled
\chapter{Graph Partitioning for Large Graphs}
\label{Chapter3}
This chapter is going to develop all the concepts related to the modifications made to the GAP framework. It contains the proposed solution and the principles to understand them.

Maybe talk a little bit about GNN in general before going to GCN
\section{Graph Convolutional Neural Networks and GraphSAGE}
As mentioned by Zhang et. al. in \cite{gcnreview}, GCN's and its variants have become a very hot topic in Machine Learning and they are being extremely used to solve plenty of problems with . Indeed, the use use of Graph Convolutional Networks (GCN) to generate node embeddings was a key factor that made the GAP framework very successful in producing good quality partitions.

Graph Convolutional Networks were originally proposed by Kifp et. al. in~\cite{gcn}. According to their authors. a single layer follows the following propagation rule:
\begin{align}
    H^{(l+1)} = f(H^{(l)}, A) &= \sigma\left(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(l)}W^{(l)}\right) \\
    & H^{(0)} = X,
\end{align}
where $W^{(l)}$ is a the learnable parameter weight matrix for the $l$-th network layer, $\hat{A}=A+I$ is the adjacency matrix with self loops, $\hat{D}$ is the diagonal node degree matrix of $\hat{A}$, $X$ is the feature matrix, and $\sigma$ is a non-linear activation function.

Downsides bottleneck
\begin{itemize}
    \item efficiency
\end{itemize}

To solve this issues in GCN's, several solutions have been proposed that deal with the downsides mentioned without loosing expression of GCN.
FastGCN
GraphSAGE 
PinSAGE

Different approaches that can be followed are found by the author in~\citep{gnnsurvey} or~\cite{hamilton}


Describe Downsides

Mention all the approaches that use Graph Neural Networks and its importance in solving graph related tasks
achieving superior performance on many graph-related tasks


Talk a little bit about traditional node embeddings approaches and its delimitations.
 Embeddings: dense vector representations
Talk about the message passing framework
Start with Grpah Convolutional Neural Networks (GCN) and its limitations. Emphasize in how GraphSAGE solve those limitations and how it extends the GCN capabilities

Benefits of GraphSAGE:As claimed by the authors of GraphSAGE in \citep{graphsage}, it has the following 
\begin{itemize}
\item It naturally generalize to unseen nodes and does not require that all nodes are present during training of the embeddings.
\end{itemize}

\section{Node Features}
Describe the entries of the feature matrix and its dimension

One of the main limitations of GAP is that it requires node features. Due to the nature of the GCNs that make use of the symmetrically normalized graph Laplacian
GNNs aim at learning node
representations by learning the similarities shared between
connected nodes. However, the expressive ability of a GNN
is highly dependent on the quality of node features
Mention here: Deep Fraud Detection on Non-attributed Graph
https://arxiv.org/pdf/2110.01171.pdf
but cited here
C. T. Duong, T. D. Hoang, H. T. H. Dang, Q. V. H. Nguyen, and
K. Aberer, “On node features for graph neural networks,” arXiv preprint
arXiv:1911.08795, 2019.
[11] H. Cui, Z. Lu, P. Li, and C. Yang, “On positional and structural node
features for graph neural networks on non-attributed graphs,” arXiv
preprint arXiv:2107.01495, 2021

Eigendecomposition and top-k-eigenvalues are the k-dimensional feature vector
Q. Huang, H. He, A. Singh, S.-N. Lim, and A. R. Benson, “Combining
label propagation and simple models out-performs graph neural net-
works,” arXiv preprint arXiv:2010.13993, 2020.

Different feature initializations
http://www.cs.emory.edu/~jyang71/files/gnnfeature.pdf

\begin{center}
    \includegraphics[scale=0.5]{partitioning_module}
\end{center}

and after playing a little bit

although GCN have been shown to extract very useful information about graph topology we need to find a different way to get that information

Some work on using Graph Neural Networks without features has been made. Use one hot representation or Initialize random feature matrices (Those methods have been shown to be equivalents and they do not generalize as mention in \citep{hamilton}. Other methods propose using applying methods as PCA to the adjacency matrix to extract the top k-eigenvalues but those approaches are computationaly very expensive, which is infeasible for large graphs and only and does not
To help capturing local information related to the graph's structure a random walk approach was chosen, have been shown efficient representation learning techniques for graphs https://arxiv.org/pdf/1901.01346.pdf, random walk kernels to produce high quality graph representations https://proceedings.neurips.cc/paper/2020/file/ba95d78a7c942571185308775a97a3a0-Paper.pdf

in particular Deep Walk to extract features about the topology of the network

According to the original paper~\cite{deepwalk}, the DeepWalk algorithm consists of two main components: a random walk generator and an update procedure. 

In the first component, a random node $v_i$ is taken uniformly at random to be the root of a random walk $\mathcal W_{v_i}$ which in its turn samples recursively from the neighbors of the last visited vertex until the maximum length $\gamma$ is reached.

As specified by Perozzi et. al.~\citep{deepwalk}, their experiments suggest that the number of walks started per vertex should be greater or equal than $\gamma=30$, the latent dimension greater or equal than $d=64$, and they fixed the sensible values of $w=10$ for the window size, and $t=40$ for the walk length . Based on those recommendations, in the experiments carried out in~\citep{deepwalk_hyper}, and the computational needs of the problem to be solved, it was found convenient to set $\gamma=60$, $d=64$, $w=15$, and $t=80$.


For the algorithm that is proposed here, the implementation by the Karate Club API~\cite{karateclub} was used.

Traditional approaches like METIS or SCOTCH implements different versions of the algorithm according to the balancedness measure, either the cardinality or the volume of the partitions. One of the great innovations presented  in~\citep{gap} is the introduction of a loss function derived from the expectation of the normalized cut. Even though they 

Ratio cut it is still used and relevant to modern research cite here https://proceedings.neurips.cc/paper/2020/file/aa108f56a10e75c1f20f27723ecac85f-Paper.pdf

or here https://www.springerprofessional.de/en/metaheuristic-approaches-for-ratio-cut-and-normalized-cut-graph-/20360832    

the authors of~\citep{gap2} proposed some modifications to GAP so it can be used for graphs without features. However the presented 

for future - Study the weighted problem
Study better features related to the problem
other ways to extract useful features for non-attributed graphs

\begin{table}
\centering
\begin{tabular}{ |p{1.75cm}||cc|  }
\hline
\multicolumn{3}{|c|}{\textbf{Computation Graphs}} \\
\hline
\hline
\textbf{Name} & \textbf{Nodes} & \textbf{Edges} \\
\hline
add20 & 2395 & 7462  \\
data & 2851 & 15093  \\
3elt & 4720 & 13722  \\
uk & 4824 & 6837  \\
add32 & 4960 & 9462  \\
bcsstk33 & 8738 & 291583  \\
whitaker3 & 9800 & 28989  \\
crack & 10240 & 30380  \\
\hline
fe\_body & 45087 & 163734  \\
t60k & 60005 & 89440  \\
wing & 62032 & 121544  \\
finan512 & 74752 & 261120 \\
\hline
fe\_rotor & 99617 & 662431  \\
598a & 110971 & 741934  \\
m14b & 214765 & 1679018	 \\
auto & 448695 & 3314611  \\
\hline
\end{tabular}
\caption{\label{tab:comp_graphs}Summary of the graphs characteristics. Taken from "The Graph Partitioning Archive~\citep{archive}"}
\end{table}

METIS was used from the networkx interface

a table for commonly used notation

\subsection{PinSAGE and Markov Chain Negative Sampling (MCNS)}