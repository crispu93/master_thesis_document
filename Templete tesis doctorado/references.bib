@article{hamilton,
    author={Hamilton, William L.},
    title={Graph Representation Learning},
    journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
    volume={14},
    number={3},
    pages={1-159},
    publisher={Morgan and Claypool},
    year={2020}
}

@inbook{gatti,
    author = {Alice Gatti and Zhixiong Hu and Tess Smidt and Esmond G. Ng and Pieter Ghysels},
    title = {Deep Learning and Spectral Embedding for Graph Partitioning},
    booktitle = {Proceedings of the 2022 SIAM Conference on Parallel Processing for Scientific Computing (PP)},
    chapter = {},
    pages = {25-36},
    doi = {10.1137/1.9781611977141.3},
    URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977141.3},
    eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611977141.3},
        abstract = { Abstract We present a graph bisection and partitioning algorithm based on graph neural networks. For each node in the graph, the network outputs probabilities for each of the partitions. The graph neural network consists of two modules: an embedding phase and a partitioning phase. The embedding phase is trained first by minimizing a loss function inspired by spectral graph theory. The partitioning module is trained through a loss function that corresponds to the expected value of the normalized cut. Both parts of the neural network rely on SAGE convolutional layers and graph coarsening using heavy edge matching. The multilevel structure of the neural network is inspired by the multigrid algorithm. Our approach generalizes very well to bigger graphs and has partition quality comparable to METIS, Scotch and spectral partitioning, with shorter runtime compared to METIS and spectral partitioning. },
    year={2022}
}

@article{gap,
  title={GAP: Generalizable Approximate Graph Partitioning Framework},
  author={Azade Nazi and Will Hang and Anna Goldie and Sujith Ravi and Azalia Mirhoseini},
  journal={ArXiv},
  year={2019},
  volume={abs/1903.00614}
}

@article{metis,
  title={A Fast and Highly Quality Multilevel Scheme for Partitioning Irregular Graphs},
  author={George Karypis and Vipin Kumar},
  journal={SIAM Journal on Scientific Computing},
  year={1999},
  volume={20},
  number={1},
  pages={359—392}
}

@inproceedings{graphsage,
    author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
    title = {Inductive Representation Learning on Large Graphs},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {1025–1035},
    numpages = {11},
    location = {Long Beach, California, USA},
    series = {NIPS'17}
    }

@article{archive,
author = {Soper, Alan and Walshaw, Chris and Cross, Mark},
year = {2004},
month = {06},
pages = {225-241},
title = {A Combined Evolutionary Search and Multilevel Optimisation Approach to Graph-Partitioning},
volume = {29},
journal = {Journal of Global Optimization},
doi = {10.1023/B:JOGO.0000042115.44455.f3}
}

@inbook{gap2,
author = {Alice Gatti and Zhixiong Hu and Tess Smidt and Esmond G. Ng and Pieter Ghysels},
title = {Deep Learning and Spectral Embedding for Graph Partitioning},
booktitle = {Proceedings of the 2022 SIAM Conference on Parallel Processing for Scientific Computing (PP)},
chapter = {},
pages = {25-36},
doi = {10.1137/1.9781611977141.3},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977141.3},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611977141.3},
    abstract = { Abstract We present a graph bisection and partitioning algorithm based on graph neural networks. For each node in the graph, the network outputs probabilities for each of the partitions. The graph neural network consists of two modules: an embedding phase and a partitioning phase. The embedding phase is trained first by minimizing a loss function inspired by spectral graph theory. The partitioning module is trained through a loss function that corresponds to the expected value of the normalized cut. Both parts of the neural network rely on SAGE convolutional layers and graph coarsening using heavy edge matching. The multilevel structure of the neural network is inspired by the multigrid algorithm. Our approach generalizes very well to bigger graphs and has partition quality comparable to METIS, Scotch and spectral partitioning, with shorter runtime compared to METIS and spectral partitioning. },
    year={2022}
}

@inproceedings{xavier,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Xavier Glorot and Yoshua Bengio},
  booktitle={AISTATS},
  year={2010}
}

@inproceedings{gap1,
  title={A DEEP LEARNING FRAMEWORK FOR GRAPH PARTITIONING},
  author={Anna Goldie and Sujith Ravi and Azalia Mirhoseini},
  journal={Published as a conference paper at ICLR 2019},
  year={2019}
}

@inproceedings{deepwalk,
author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
title = {DeepWalk: Online Learning of Social Representations},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623732},
doi = {10.1145/2623330.2623732},
abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data.DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {701–710},
numpages = {10},
keywords = {latent representations, deep learning, social networks, network classification, learning with partial labels, online learning},
location = {New York, New York, USA},
series = {KDD '14}
}

@inbook{deepwalk_hyper,
author = {Yunfang, Chen and Wang, Li and Qi, Dehao and Zhang, Wei},
year = {2020},
month = {08},
pages = {568-583},
title = {Community Detection Based on DeepWalk in Large Scale Networks},
isbn = {978-981-15-7529-7},
doi = {10.1007/978-981-15-7530-3_43},
booktitle={Communications in Computer and Information Science },
publisher = {Springer Publishing Company, Incorporated}
}
  
@inproceedings{karateclub,
               title = {{Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs}},
               author = {Benedek Rozemberczki and Oliver Kiss and Rik Sarkar},
               year = {2020},
               pages = {3125–3132},
               booktitle = {Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20)},
               organization = {ACM},
}

@inproceedings{compressgraphs,
  title={Partition and Code: learning how to compress graphs},
  author={Giorgos Bouritsas and Andreas Loukas and Nikolaos Karalias and Michael M. Bronstein},
  booktitle={NeurIPS},
  year={2021}
}

@article{deviceplacement,
  title={Graph Representation Matters in Device Placement},
  author={Milko Mitropolitsky and Zainab Abbas and Amir H. Payberah},
  journal={Proceedings of the Workshop on Distributed Infrastructures for Deep Learning},
  year={2020}
}

@Article{islanding,
AUTHOR = {Sun, Zhonglin and Spyridis, Yannis and Lagkas, Thomas and Sesis, Achilleas and Efstathopoulos, Georgios and Sarigiannidis, Panagiotis},
TITLE = {End-to-End Deep Graph Convolutional Neural Network Approach for Intentional Islanding in Power Systems Considering Load-Generation Balance},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1650},
URL = {https://www.mdpi.com/1424-8220/21/5/1650},
PubMedID = {33673514},
ISSN = {1424-8220},
ABSTRACT = {Intentional islanding is a corrective procedure that aims to protect the stability of the power system during an emergency, by dividing the grid into several partitions and isolating the elements that would cause cascading failures. This paper proposes a deep learning method to solve the problem of intentional islanding in an end-to-end manner. Two types of loss functions are examined for the graph partitioning task, and a loss function is added on the deep learning model, aiming to minimise the load-generation imbalance in the formed islands. In addition, the proposed solution incorporates a technique for merging the independent buses to their nearest neighbour in case there are isolated buses after the clusterisation, improving the final result in cases of large and complex systems. Several experiments demonstrate that the introduced deep learning method provides effective clustering results for intentional islanding, managing to keep the power imbalance low and creating stable islands. Finally, the proposed method is dynamic, relying on real-time system conditions to calculate the result.},
DOI = {10.3390/s21051650}
}

@ARTICLE{imagesegmentation,

  author={Grady, L. and Schwartz, E.L.},

  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 

  title={Isoperimetric graph partitioning for image segmentation}, 

  year={2006},

  volume={28},

  number={3},

  pages={469-475},

  doi={10.1109/TPAMI.2006.57}}

@article{gnnsurvey,
author = {Zhang, Ziwei and Cui, Peng and Zhu, Wenwu},
year = {2020},
month = {03},
pages = {1-1},
title = {Deep Learning on Graphs: A Survey},
volume = {PP},
journal = {IEEE Transactions on Knowledge and Data Engineering},
doi = {10.1109/TKDE.2020.2981333}
}

@article{gcnreview,
author = {Zhang, Si and Tong, Hanghang and Xu, Jiejun and Maciejewski, Ross},
year = {2019},
month = {11},
pages = {},
title = {Graph convolutional networks: a comprehensive review},
volume = {6},
journal = {Computational Social Networks},
doi = {10.1186/s40649-019-0069-y}
}

@article{gcn,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}

@article{nodefeatures,
  author    = {Chen Wang and
               Yingtong Dou and
               Min Chen and
               Jia Chen and
               Zhiwei Liu and
               Philip S. Yu},
  title     = {Deep Fraud Detection on Non-attributed Graph},
  journal   = {CoRR},
  volume    = {abs/2110.01171},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.01171},
  eprinttype = {arXiv},
  eprint    = {2110.01171},
  timestamp = {Fri, 08 Oct 2021 15:47:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-01171.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Book{appcombinatorial,
author = {Korte, Bernhard and Vygen, Jens},
title = {Combinatorial Optimization: Theory and Algorithms},
year = {2012},
isbn = {3642244874},
publisher = {Springer Publishing Company, Incorporated},
edition = {5th},
abstract = {This comprehensive textbook on combinatorial optimization places specialemphasis on theoretical results and algorithms with provably goodperformance, in contrast to heuristics. It is based on numerous courses on combinatorial optimization and specialized topics, mostly at graduate level. This book reviews the fundamentals, covers the classical topics (paths, flows, matching, matroids, NP-completeness, approximation algorithms) in detail, and proceeds to advanced and recent topics, some of which have not appeared in a textbook before. Throughout,it contains complete but concise proofs, and also provides numerousexercises and references. This fifth edition has again been updated, revised, and significantlyextended, with more than 60 new exercises and new material on varioustopics, including Cayley's formula, blocking flows, fasterb-matching separation, multidimensional knapsack, multicommoditymax-flow min-cut ratio, and sparsest cut. Thus, this book represents the state of the art of combinatorial optimization.}
}

@misc{brilliant, title={Combinatorial optimization}, url={https://brilliant.org/wiki/combinatorial-optimization/}, howpublished = {\url{https://brilliant.org/wiki/combinatorial-optimization/}}, journal={Brilliant Math and Science Wiki}, author={Henry Maltby and Eli Ross }, year = {2022} }

@phdthesis{fernandes, title={Algorithms and Models For Combinatorial Optimization Problems}, author={Fernandes, Albert Einstein Muritiba}, year={2010}} 