@article{hamilton,
    author={Hamilton, William L.},
    title={Graph Representation Learning},
    journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
    volume={14},
    number={3},
    pages={1-159},
    publisher={Morgan and Claypool},
    year={2020}
}

@inbook{gatti,
    author = {Alice Gatti and Zhixiong Hu and Tess Smidt and Esmond G. Ng and Pieter Ghysels},
    title = {Deep Learning and Spectral Embedding for Graph Partitioning},
    booktitle = {Proceedings of the 2022 SIAM Conference on Parallel Processing for Scientific Computing (PP)},
    chapter = {},
    pages = {25-36},
    doi = {10.1137/1.9781611977141.3},
    URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977141.3},
    eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611977141.3},
        abstract = { Abstract We present a graph bisection and partitioning algorithm based on graph neural networks. For each node in the graph, the network outputs probabilities for each of the partitions. The graph neural network consists of two modules: an embedding phase and a partitioning phase. The embedding phase is trained first by minimizing a loss function inspired by spectral graph theory. The partitioning module is trained through a loss function that corresponds to the expected value of the normalized cut. Both parts of the neural network rely on SAGE convolutional layers and graph coarsening using heavy edge matching. The multilevel structure of the neural network is inspired by the multigrid algorithm. Our approach generalizes very well to bigger graphs and has partition quality comparable to METIS, Scotch and spectral partitioning, with shorter runtime compared to METIS and spectral partitioning. },
    year={2022}
}

@article{gap,
  title={GAP: Generalizable Approximate Graph Partitioning Framework},
  author={Azade Nazi and Will Hang and Anna Goldie and Sujith Ravi and Azalia Mirhoseini},
  journal={ArXiv},
  year={2019},
  volume={abs/1903.00614}
}

@article{metis,
  title={A Fast and Highly Quality Multilevel Scheme for Partitioning Irregular Graphs},
  author={George Karypis and Vipin Kumar},
  journal={SIAM Journal on Scientific Computing},
  year={1999},
  volume={20},
  number={1},
  pages={359—392}
}

@inproceedings{graphsage,
    author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
    title = {Inductive Representation Learning on Large Graphs},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {1025–1035},
    numpages = {11},
    location = {Long Beach, California, USA},
    series = {NIPS'17}
    }

@article{archive,
author = {Soper, Alan and Walshaw, Chris and Cross, Mark},
year = {2004},
month = {06},
pages = {225-241},
title = {A Combined Evolutionary Search and Multilevel Optimisation Approach to Graph-Partitioning},
volume = {29},
journal = {Journal of Global Optimization},
doi = {10.1023/B:JOGO.0000042115.44455.f3}
}

@inbook{gap2,
author = {Alice Gatti and Zhixiong Hu and Tess Smidt and Esmond G. Ng and Pieter Ghysels},
title = {Deep Learning and Spectral Embedding for Graph Partitioning},
booktitle = {Proceedings of the 2022 SIAM Conference on Parallel Processing for Scientific Computing (PP)},
chapter = {},
pages = {25-36},
doi = {10.1137/1.9781611977141.3},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977141.3},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611977141.3},
    abstract = { Abstract We present a graph bisection and partitioning algorithm based on graph neural networks. For each node in the graph, the network outputs probabilities for each of the partitions. The graph neural network consists of two modules: an embedding phase and a partitioning phase. The embedding phase is trained first by minimizing a loss function inspired by spectral graph theory. The partitioning module is trained through a loss function that corresponds to the expected value of the normalized cut. Both parts of the neural network rely on SAGE convolutional layers and graph coarsening using heavy edge matching. The multilevel structure of the neural network is inspired by the multigrid algorithm. Our approach generalizes very well to bigger graphs and has partition quality comparable to METIS, Scotch and spectral partitioning, with shorter runtime compared to METIS and spectral partitioning. },
    year={2022}
}

@inproceedings{xavier,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Xavier Glorot and Yoshua Bengio},
  booktitle={AISTATS},
  year={2010}
}

@inproceedings{gap1,
  title={A DEEP LEARNING FRAMEWORK FOR GRAPH PARTITIONING},
  author={Anna Goldie and Sujith Ravi and Azalia Mirhoseini},
  journal={Published as a conference paper at ICLR 2019},
  year={2019}
}

@inproceedings{deepwalk,
author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
title = {DeepWalk: Online Learning of Social Representations},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623732},
doi = {10.1145/2623330.2623732},
abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data.DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {701–710},
numpages = {10},
keywords = {latent representations, deep learning, social networks, network classification, learning with partial labels, online learning},
location = {New York, New York, USA},
series = {KDD '14}
}

@inbook{deepwalk_hyper,
author = {Yunfang, Chen and Wang, Li and Qi, Dehao and Zhang, Wei},
year = {2020},
month = {08},
pages = {568-583},
title = {Community Detection Based on DeepWalk in Large Scale Networks},
isbn = {978-981-15-7529-7},
doi = {10.1007/978-981-15-7530-3_43},
booktitle={Communications in Computer and Information Science },
publisher = {Springer Publishing Company, Incorporated}
}
  
@inproceedings{karateclub,
               title = {{Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs}},
               author = {Benedek Rozemberczki and Oliver Kiss and Rik Sarkar},
               year = {2020},
               pages = {3125–3132},
               booktitle = {Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20)},
               organization = {ACM},
}

@inproceedings{compressgraphs,
  title={Partition and Code: learning how to compress graphs},
  author={Giorgos Bouritsas and Andreas Loukas and Nikolaos Karalias and Michael M. Bronstein},
  booktitle={NeurIPS},
  year={2021}
}

@article{deviceplacement,
  title={Graph Representation Matters in Device Placement},
  author={Milko Mitropolitsky and Zainab Abbas and Amir H. Payberah},
  journal={Proceedings of the Workshop on Distributed Infrastructures for Deep Learning},
  year={2020}
}

@Article{islanding,
AUTHOR = {Sun, Zhonglin and Spyridis, Yannis and Lagkas, Thomas and Sesis, Achilleas and Efstathopoulos, Georgios and Sarigiannidis, Panagiotis},
TITLE = {End-to-End Deep Graph Convolutional Neural Network Approach for Intentional Islanding in Power Systems Considering Load-Generation Balance},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1650},
URL = {https://www.mdpi.com/1424-8220/21/5/1650},
PubMedID = {33673514},
ISSN = {1424-8220},
ABSTRACT = {Intentional islanding is a corrective procedure that aims to protect the stability of the power system during an emergency, by dividing the grid into several partitions and isolating the elements that would cause cascading failures. This paper proposes a deep learning method to solve the problem of intentional islanding in an end-to-end manner. Two types of loss functions are examined for the graph partitioning task, and a loss function is added on the deep learning model, aiming to minimise the load-generation imbalance in the formed islands. In addition, the proposed solution incorporates a technique for merging the independent buses to their nearest neighbour in case there are isolated buses after the clusterisation, improving the final result in cases of large and complex systems. Several experiments demonstrate that the introduced deep learning method provides effective clustering results for intentional islanding, managing to keep the power imbalance low and creating stable islands. Finally, the proposed method is dynamic, relying on real-time system conditions to calculate the result.},
DOI = {10.3390/s21051650}
}

@ARTICLE{imagesegmentation,

  author={Grady, L. and Schwartz, E.L.},

  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 

  title={Isoperimetric graph partitioning for image segmentation}, 

  year={2006},

  volume={28},

  number={3},

  pages={469-475},

  doi={10.1109/TPAMI.2006.57}}

@article{gnnsurvey,
author = {Zhang, Ziwei and Cui, Peng and Zhu, Wenwu},
year = {2020},
month = {03},
pages = {1-1},
title = {Deep Learning on Graphs: A Survey},
volume = {PP},
journal = {IEEE Transactions on Knowledge and Data Engineering},
doi = {10.1109/TKDE.2020.2981333}
}

@article{gcnreview,
author = {Zhang, Si and Tong, Hanghang and Xu, Jiejun and Maciejewski, Ross},
year = {2019},
month = {11},
pages = {},
title = {Graph convolutional networks: a comprehensive review},
volume = {6},
journal = {Computational Social Networks},
doi = {10.1186/s40649-019-0069-y}
}

@article{gcn,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}

@article{nodefeatures,
  author    = {Chen Wang and
               Yingtong Dou and
               Min Chen and
               Jia Chen and
               Zhiwei Liu and
               Philip S. Yu},
  title     = {Deep Fraud Detection on Non-attributed Graph},
  journal   = {CoRR},
  volume    = {abs/2110.01171},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.01171},
  eprinttype = {arXiv},
  eprint    = {2110.01171},
  timestamp = {Fri, 08 Oct 2021 15:47:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-01171.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Book{appcombinatorial,
author = {Korte, Bernhard and Vygen, Jens},
title = {Combinatorial Optimization: Theory and Algorithms},
year = {2012},
isbn = {3642244874},
publisher = {Springer Publishing Company, Incorporated},
edition = {5th},
abstract = {This comprehensive textbook on combinatorial optimization places specialemphasis on theoretical results and algorithms with provably goodperformance, in contrast to heuristics. It is based on numerous courses on combinatorial optimization and specialized topics, mostly at graduate level. This book reviews the fundamentals, covers the classical topics (paths, flows, matching, matroids, NP-completeness, approximation algorithms) in detail, and proceeds to advanced and recent topics, some of which have not appeared in a textbook before. Throughout,it contains complete but concise proofs, and also provides numerousexercises and references. This fifth edition has again been updated, revised, and significantlyextended, with more than 60 new exercises and new material on varioustopics, including Cayley's formula, blocking flows, fasterb-matching separation, multidimensional knapsack, multicommoditymax-flow min-cut ratio, and sparsest cut. Thus, this book represents the state of the art of combinatorial optimization.}
}

@misc{brilliant, 
    title={Combinatorial optimization}, 
    url={https://brilliant.org/wiki/combinatorial-optimization/}, 
    howpublished = {\url{ https://brilliant.org/wiki/combinatorial-optimization/ }}, 
    journal={Brilliant Math and Science Wiki}, 
    author={Henry Maltby and Eli Ross }, 
    year = {2022} 
}

@phdthesis{fernandes, 
title={Algorithms and Models For Combinatorial Optimization Problems}, 
author={Fernandes, Albert Einstein Muritiba}, 
year={2010}
}

@book{schrijver-book,
  added-at = {2007-07-05T16:17:35.000+0200},
  author = {Schrijver, A.},
  biburl = {https://www.bibsonomy.org/bibtex/2496d0012f9b295acbef270a129061375/jleny},
  description = {bandit problems},
  interhash = {dfbeb3a87380195540f44a10e9995230},
  intrahash = {496d0012f9b295acbef270a129061375},
  keywords = {imported},
  publisher = {Springer},
  timestamp = {2007-07-05T16:17:37.000+0200},
  title = {Combinatorial Optimization - Polyhedra and Efficiency},
  year = {2003}
}

@Inbook{integeroptimization,
author="Hoffman, Karla L.
and Ralphs, Ted K.",
editor="Gass, Saul I.
and Fu, Michael C.",
title="Integer and Combinatorial Optimization",
bookTitle="Encyclopedia of Operations Research and Management Science",
year="2013",
publisher="Springer US",
address="Boston, MA",
pages="771--783",
isbn="978-1-4419-1153-7",
doi="10.1007/978-1-4419-1153-7_129",
url="https://doi.org/10.1007/978-1-4419-1153-7_129"
}

@article{branchbound,
author = {Morrison, David R. and Jacobson, Sheldon H. and Sauppe, Jason J. and Sewell, Edward C.},
title = {Branch-and-Bound Algorithms},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {19},
number = {C},
issn = {1572-5286},
url = {https://doi.org/10.1016/j.disopt.2016.01.005},
doi = {10.1016/j.disopt.2016.01.005},
abstract = {The branch-and-bound (B&B) algorithmic framework has been used successfully to find exact solutions for a wide array of optimization problems. B&B uses a tree search strategy to implicitly enumerate all possible solutions to a given problem, applying pruning rules to eliminate regions of the search space that cannot lead to a better solution. There are three algorithmic components in B&B that can be specified by the user to fine-tune the behavior of the algorithm. These components are the search strategy, the branching strategy, and the pruning rules. This survey presents a description of recent research advances in the design of B&B algorithms, particularly with regards to these three components. Moreover, three future research directions are provided in order to motivate further exploration in these areas.},
journal = {Discret. Optim.},
month = {feb},
pages = {79–102},
numpages = {24},
keywords = {Cyclic best first search, 90C10, 90C57, Discrete optimization, Survey, 90C27, Branch-and-bound, Integer programming, 90-02, Search strategies}
}

@book{heuristics,
author = {Pearl, Judea},
title = {Heuristics:  Intelligent Search Strategies for Computer Problem Solving},
year = {1984},
isbn = {0201055945},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA}
}

@misc{branchimage,
  title = {Introduction into the Modeling and Optimization of Linear Systems},
  howpublished = {\url{https://hegyhati.github.io/IMOLS/}},
  note = {Accessed: 2010-09-30},
  author = {Máté Hegyháti},
  year = {2022}
}

@article{complexnetworks,
author = {Dirk Brockmann  and Dirk Helbing },
title = {The Hidden Geometry of Complex, Network-Driven Contagion Phenomena},
journal = {Science},
volume = {342},
number = {6164},
pages = {1337-1342},
year = {2013},
doi = {10.1126/science.1245200},
URL = {https://www.science.org/doi/abs/10.1126/science.1245200},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1245200},
abstract = {In combating the global spread of an emerging infectious disease, answers must be obtained to three crucial questions: Where did the disease emerge? Where will it go next? When will it arrive? Brockmann and Helbing (p. 1337; see the Perspective by McLean) analyzed disease spread via the “effective distance” rather than geographical distance, wherein two locations that are connected by a strong link are effectively close. The approach was successfully applied to predict disease arrival times or disease source using data from the the 2003 SARS viral epidemic, 2009 H1N1 influenza pandemic, and the 2011 foodborne enterohaemorrhagic Escherichia coli outbreak in Germany. A model based on effective rather than geographical distance can reveal the origin, timing, and likely spread of epidemics. [Also see Perspective by McLean] The global spread of epidemics, rumors, opinions, and innovations are complex, network-driven dynamic processes. The combined multiscale nature and intrinsic heterogeneity of the underlying networks make it difficult to develop an intuitive understanding of these processes, to distinguish relevant from peripheral factors, to predict their time course, and to locate their origin. However, we show that complex spatiotemporal patterns can be reduced to surprisingly simple, homogeneous wave propagation patterns, if conventional geographic distance is replaced by a probabilistically motivated effective distance. In the context of global, air-traffic–mediated epidemics, we show that effective distance reliably predicts disease arrival times. Even if epidemiological parameters are unknown, the method can still deliver relative arrival times. The approach can also identify the spatial origin of spreading processes and successfully be applied to data of the worldwide 2009 H1N1 influenza pandemic and 2003 SARS epidemic.}
}

@article{bettersuited,
  title={Fuzzy lattice reasoning (FLR) type neural computation for weighted graph partitioning},
  author={Vassilis G. Kaburlasos and Lefteris Moussiades and Athena Vakali},
  journal={Neurocomputing},
  year={2009},
  volume={72},
  pages={2121-2133}
}

@inproceedings{sparcification,
author = {Spielman, Daniel A. and Teng, Shang-Hua},
title = {Nearly-Linear Time Algorithms for Graph Partitioning, Graph Sparsification, and Solving Linear Systems},
year = {2004},
isbn = {1581138520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1007352.1007372},
doi = {10.1145/1007352.1007372},
abstract = {We present algorithms for solving symmetric, diagonally-dominant linear systems to accuracy ε in time linear in their number of non-zeros and log (κf (A) ε), where κf (A) is the condition number of the matrix defining the linear system. Our algorithm applies the preconditioned Chebyshev iteration with preconditioners designed using nearly-linear time algorithms for graph sparsification and graph partitioning.},
booktitle = {Proceedings of the Thirty-Sixth Annual ACM Symposium on Theory of Computing},
pages = {81–90},
numpages = {10},
keywords = {preconditioners, graph partitioning, graph sparsification},
location = {Chicago, IL, USA},
series = {STOC '04}
}

@article{nesteddissection,
  title={Graph Partitioning and Sparse Matrix Ordering using Reinforcement Learning},
  author={Alice Gatti and Zhixiong Hu and Pieter Ghysels and Esmond G. Ng and Tess E. Smidt},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.03546}
}

@ARTICLE{matrixordering,
  author={Gupta, A.},
  journal={IBM Journal of Research and Development}, 
  title={Fast and effective algorithms for graph partitioning and sparse-matrix ordering}, 
  year={1997},
  volume={41},
  number={1.2},
  pages={171-183},
  doi={10.1147/rd.411.0171}
}

@article{jostle,
author = {Walshaw, Chris},
year = {2005},
month = {08},
pages = {},
howpublished = {\url{https://chriswalshaw.co.uk/jostle/jostle-exe.pdf}},
title = {Contents The JOSTLE executable user guide: Version 3.1}
}

@article{local_clustering,
author = {Spielman, Daniel A. and Teng, Shang-Hua},
title = {A Local Clustering Algorithm for Massive Graphs and Its Application to Nearly Linear Time Graph Partitioning},
journal = {SIAM Journal on Computing},
volume = {42},
number = {1},
pages = {1-26},
year = {2013},
doi = {10.1137/080744888},

URL = { 
        https://doi.org/10.1137/080744888
    
},
eprint = { 
        https://doi.org/10.1137/080744888
    
}
,
    abstract = { We study the design of local algorithms for massive graphs. A local graph algorithm is one that finds a solution containing or near a given vertex without looking at the whole graph. We present a local clustering algorithm. Our algorithm finds a good cluster---a subset of vertices whose internal connections are significantly richer than its external connections---near a given vertex. The running time of our algorithm, when it finds a nonempty local cluster, is nearly linear in the size of the cluster it outputs. The running time of our algorithm also depends polylogarithmically on the size of the graph and polynomially on the conductance of the cluster it produces. Our clustering algorithm could be a useful primitive for handling massive graphs, such as social networks and web-graphs. As an application of this clustering algorithm, we present a partitioning algorithm that finds an approximate sparsest cut with nearly optimal balance. Our algorithm takes time nearly linear in the number edges of the graph. Using the partitioning algorithm of this paper, we have designed a nearly linear time algorithm for constructing spectral sparsifiers of graphs, which we in turn use in a nearly linear time algorithm for solving linear systems in symmetric, diagonally dominant matrices. The linear system solver also leads to a nearly linear time algorithm for approximating the second-smallest eigenvalue and corresponding eigenvector of the Laplacian matrix of a graph. These other results are presented in two companion papers. }
}

@article{sparsification,
author = {Spielman, Daniel A. and Teng, Shang-Hua},
title = {Spectral Sparsification of Graphs},
journal = {SIAM Journal on Computing},
volume = {40},
number = {4},
pages = {981-1025},
year = {2011},
doi = {10.1137/08074489X},

URL = { 
        https://doi.org/10.1137/08074489X
    
},
eprint = { 
        https://doi.org/10.1137/08074489X
    
}
,
    abstract = { We introduce a new notion of graph sparsification based on spectral similarity of graph Laplacians: spectral sparsification requires that the Laplacian quadratic form of the sparsifier approximate that of the original. This is equivalent to saying that the Laplacian of the sparsifier is a good preconditioner for the Laplacian of the original. We prove that every graph has a spectral sparsifier of nearly linear size. Moreover, we present an algorithm that produces spectral sparsifiers in time \$O(m\log^{c}m)\$, where m is the number of edges in the original graph and c is some absolute constant. This construction is a key component of a nearly linear time algorithm for solving linear equations in diagonally dominant matrices. Our sparsification algorithm makes use of a nearly linear time algorithm for graph partitioning that satisfies a strong guarantee: if the partition it outputs is very unbalanced, then the larger part is contained in a subgraph of high conductance. }
}

@article{linearsystems,
    author = {Spielman, Daniel A. and Teng, Shang-Hua},
    title = {Nearly Linear Time Algorithms for Preconditioning and Solving Symmetric, Diagonally Dominant Linear Systems},
    journal = {SIAM Journal on Matrix Analysis and Applications},
    volume = {35},
    number = {3},
    pages = {835-885},
    year = {2014},
    doi = {10.1137/090771430},
    URL = { 
        https://doi.org/10.1137/090771430
    
},
    eprint = { 
        https://doi.org/10.1137/090771430
    
}
}

@Inbook{clustering,
author="Erciyes, K.",
title="Graph Partitioning and ClusteringGraph partitioning",
bookTitle="Algebraic Graph Algorithms: A Practical Guide Using Python",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="199--218",
abstract="Partitioning and clustering are two main operations on graphs that find a wide range of applications. Graph partitioning aims at balanced partitions with minimum interactions between partitions. However, graph clustering algorithms attempt to discover densely populated regions of graphs. We review algebraic algorithms for these problems and provide Python implementations of these algorithms in this chapter.",
isbn="978-3-030-87886-3",
doi="10.1007/978-3-030-87886-3_11",
url="https://doi.org/10.1007/978-3-030-87886-3_11"
}

@inproceedings{encoderlimitations,
    title={How Powerful are Graph Neural Networks?},
    author={Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=ryGs6iA5Km}
}

@inproceedings{FastGCN,
  author    = {Jie Chen and
               Tengfei Ma and
               Cao Xiao},
  title     = {FastGCN: Fast Learning with Graph Convolutional Networks via Importance
               Sampling},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=rytstxWAW},
  timestamp = {Mon, 31 May 2021 15:36:24 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ChenMX18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{pinsage,
author = {Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L. and Leskovec, Jure},
title = {Graph Convolutional Neural Networks for Web-Scale Recommender Systems},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219890},
doi = {10.1145/3219819.3219890},
abstract = {Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains an unsolved challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. Overall, we can train on and embed graphs that are four orders of magnitude larger than typical GCN implementations. We show how GCN embeddings can be used to make high-quality recommendations in various settings at Pinterest, which has a massive underlying graph with 3 billion nodes representing pins and boards, and 17 billion edges. According to offline metrics, user studies, as well as A/B tests, our approach generates higher-quality recommendations than comparable deep learning based systems. To our knowledge, this is by far the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {974–983},
numpages = {10},
keywords = {scalability, recommender systems, graph convolutional networks, deep learning},
location = {London, United Kingdom},
series = {KDD '18}
}

@misc{unsupervisedloss, 
    title={GraphSAGE and inductive representation learning}, 
    url={https://medium.com/analytics-vidhya/ohmygraphs-graphsage-and-inductive-representation-learning-ea26d2835331}, 
    howpublished = {\url{ shorturl.at/oDEIY }}, 
    journal={Medium and analytics-vidhya}, 
    author={Nabila Abraham}, 
    year = {2020} 
}

@article{frauddetection,
  author    = {Chen Wang and
               Yingtong Dou and
               Min Chen and
               Jia Chen and
               Zhiwei Liu and
               Philip S. Yu},
  title     = {Deep Fraud Detection on Non-attributed Graph},
  journal   = {CoRR},
  volume    = {abs/2110.01171},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.01171},
  eprinttype = {arXiv},
  eprint    = {2110.01171},
  timestamp = {Fri, 08 Oct 2021 15:47:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-01171.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{nonattributed,
 title={A simple yet effective baseline for non-attribute graph classification},
 author={Cai, Chen and Wang, Yusu},
 journal={arXiv preprint arXiv:1811.03508},
 year={2018}
}
%ICLR 2019 workshop on Representation learning on graphs and manifolds

@inproceedings{imagenodefeatures,
author = {Gao, Hongyang and Wang, Zhengyang and Ji, Shuiwang},
title = {Large-Scale Learnable Graph Convolutional Networks},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219947},
doi = {10.1145/3219819.3219947},
abstract = {Convolutional neural networks (CNNs) have achieved great success on grid-like data such as images, but face tremendous challenges in learning from more generic data such as graphs. In CNNs, the trainable local filters enable the automatic extraction of high-level features. The computation with filters requires a fixed number of ordered units in the receptive fields. However, the number of neighboring units is neither fixed nor are they ordered in generic graphs, thereby hindering the applications of convolutional operations. Here, we address these challenges by proposing the learnable graph convolutional layer (LGCL). LGCL automatically selects a fixed number of neighboring nodes for each feature based on value ranking in order to transform graph data into grid-like structures in 1-D format, thereby enabling the use of regular convolutional operations on generic graphs. To enable model training on large-scale graphs, we propose a sub-graph training method to reduce the excessive memory and computational resource requirements suffered by prior methods on graph convolutions. Our experimental results on node classification tasks in both transductive and inductive learning settings demonstrate that our methods can achieve consistently better performance on the Cora, Citeseer, Pubmed citation network, and protein-protein interaction network datasets. Our results also indicate that the proposed methods using sub-graph training strategy are more efficient as compared to prior approaches.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1416–1424},
numpages = {9},
keywords = {large-scale learning, graph convolutional networks, deep learning, graph mining},
location = {London, United Kingdom},
series = {KDD '18}
}

@article{onnodefeatures,
  title={On Node Features for Graph Neural Networks},
  author={Chi Thang Duong and Thanh Dat Hoang and Haikun Dang and Quoc Viet Hung Nguyen and Karl Aberer},
  journal={ArXiv},
  year={2019},
  volume={abs/1911.08795}
}

@article{onpositional,
  title={On Positional and Structural Node Features for Graph Neural Networks on Non-attributed Graphs},
  author={Hejie Cui and Zijie Lu and Pan Li and Carl Yang},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.01495}
}

@article{eigen,
author = {Chaudhuri, Kamalika and Chung, Fan and Tsiatas, Alexander},
year = {2012},
month = {01},
pages = {35.1-35.23},
title = {Spectral Clustering of Graphs with General Degrees in the Extended Planted Partition Model},
volume = {23},
journal = {Journal of Machine Learning Research}
}

@inproceedings{eigen2,
title={Combining Label Propagation and Simple Models out-performs Graph Neural Networks},
author={Qian Huang and Horace He and Abhay Singh and Ser-Nam Lim and Austin Benson},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=8E1-f3VhX1o}
}