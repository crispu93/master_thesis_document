@article{hamilton,
    author={Hamilton, William L.},
    title={Graph Representation Learning},
    journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
    volume={14},
    number={3},
    pages={1-159},
    publisher={Morgan and Claypool},
    year={2020}
}

@inbook{gatti,
    author = {Alice Gatti and Zhixiong Hu and Tess Smidt and Esmond G. Ng and Pieter Ghysels},
    title = {Deep Learning and Spectral Embedding for Graph Partitioning},
    booktitle = {Proceedings of the 2022 SIAM Conference on Parallel Processing for Scientific Computing (PP)},
    chapter = {},
    pages = {25-36},
    doi = {10.1137/1.9781611977141.3},
    URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977141.3},
    eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611977141.3},
        abstract = { Abstract We present a graph bisection and partitioning algorithm based on graph neural networks. For each node in the graph, the network outputs probabilities for each of the partitions. The graph neural network consists of two modules: an embedding phase and a partitioning phase. The embedding phase is trained first by minimizing a loss function inspired by spectral graph theory. The partitioning module is trained through a loss function that corresponds to the expected value of the normalized cut. Both parts of the neural network rely on SAGE convolutional layers and graph coarsening using heavy edge matching. The multilevel structure of the neural network is inspired by the multigrid algorithm. Our approach generalizes very well to bigger graphs and has partition quality comparable to METIS, Scotch and spectral partitioning, with shorter runtime compared to METIS and spectral partitioning. },
    year={2022}
}

@article{gap,
  title={GAP: Generalizable Approximate Graph Partitioning Framework},
  author={Azade Nazi and Will Hang and Anna Goldie and Sujith Ravi and Azalia Mirhoseini},
  journal={ArXiv},
  year={2019},
  volume={abs/1903.00614}
}

@article{metis,
  title={A Fast and Highly Quality Multilevel Scheme for Partitioning Irregular Graphs},
  author={George Karypis and Vipin Kumar},
  journal={SIAM Journal on Scientific Computing},
  year={1999},
  volume={20},
  number={1},
  pages={359—392}
}

@inproceedings{graphsage,
    author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
    title = {Inductive Representation Learning on Large Graphs},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {1025–1035},
    numpages = {11},
    location = {Long Beach, California, USA},
    series = {NIPS'17}
    }

@article{archive,
author = {Soper, Alan and Walshaw, Chris and Cross, Mark},
year = {2004},
month = {06},
pages = {225-241},
title = {A Combined Evolutionary Search and Multilevel Optimisation Approach to Graph-Partitioning},
volume = {29},
journal = {Journal of Global Optimization},
doi = {10.1023/B:JOGO.0000042115.44455.f3}
}

@inbook{gap2,
author = {Alice Gatti and Zhixiong Hu and Tess Smidt and Esmond G. Ng and Pieter Ghysels},
title = {Deep Learning and Spectral Embedding for Graph Partitioning},
booktitle = {Proceedings of the 2022 SIAM Conference on Parallel Processing for Scientific Computing (PP)},
chapter = {},
pages = {25-36},
doi = {10.1137/1.9781611977141.3},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977141.3},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611977141.3},
    abstract = { Abstract We present a graph bisection and partitioning algorithm based on graph neural networks. For each node in the graph, the network outputs probabilities for each of the partitions. The graph neural network consists of two modules: an embedding phase and a partitioning phase. The embedding phase is trained first by minimizing a loss function inspired by spectral graph theory. The partitioning module is trained through a loss function that corresponds to the expected value of the normalized cut. Both parts of the neural network rely on SAGE convolutional layers and graph coarsening using heavy edge matching. The multilevel structure of the neural network is inspired by the multigrid algorithm. Our approach generalizes very well to bigger graphs and has partition quality comparable to METIS, Scotch and spectral partitioning, with shorter runtime compared to METIS and spectral partitioning. },
    year={2022}
}

@inproceedings{xavier,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Xavier Glorot and Yoshua Bengio},
  booktitle={AISTATS},
  year={2010}
}

@inproceedings{gap1,
  title={A DEEP LEARNING FRAMEWORK FOR GRAPH PARTITIONING},
  author={Anna Goldie and Sujith Ravi and Azalia Mirhoseini},
  journal={Published as a conference paper at ICLR 2019},
  year={2019}
}

@inproceedings{deepwalk,
author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
title = {DeepWalk: Online Learning of Social Representations},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623732},
doi = {10.1145/2623330.2623732},
abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data.DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {701–710},
numpages = {10},
keywords = {latent representations, deep learning, social networks, network classification, learning with partial labels, online learning},
location = {New York, New York, USA},
series = {KDD '14}
}

@inbook{deepwalk_hyper,
author = {Yunfang, Chen and Wang, Li and Qi, Dehao and Zhang, Wei},
year = {2020},
month = {08},
pages = {568-583},
title = {Community Detection Based on DeepWalk in Large Scale Networks},
isbn = {978-981-15-7529-7},
doi = {10.1007/978-981-15-7530-3_43},
booktitle={Communications in Computer and Information Science },
publisher = {Springer Publishing Company, Incorporated}
}
  
@inproceedings{karateclub,
               title = {{Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs}},
               author = {Benedek Rozemberczki and Oliver Kiss and Rik Sarkar},
               year = {2020},
               pages = {3125–3132},
               booktitle = {Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20)},
               organization = {ACM},
}

@inproceedings{compressgraphs,
  title={Partition and Code: learning how to compress graphs},
  author={Giorgos Bouritsas and Andreas Loukas and Nikolaos Karalias and Michael M. Bronstein},
  booktitle={NeurIPS},
  year={2021}
}

@article{deviceplacement,
  title={Graph Representation Matters in Device Placement},
  author={Milko Mitropolitsky and Zainab Abbas and Amir H. Payberah},
  journal={Proceedings of the Workshop on Distributed Infrastructures for Deep Learning},
  year={2020}
}

@Article{islanding,
AUTHOR = {Sun, Zhonglin and Spyridis, Yannis and Lagkas, Thomas and Sesis, Achilleas and Efstathopoulos, Georgios and Sarigiannidis, Panagiotis},
TITLE = {End-to-End Deep Graph Convolutional Neural Network Approach for Intentional Islanding in Power Systems Considering Load-Generation Balance},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {5},
ARTICLE-NUMBER = {1650},
URL = {https://www.mdpi.com/1424-8220/21/5/1650},
PubMedID = {33673514},
ISSN = {1424-8220},
ABSTRACT = {Intentional islanding is a corrective procedure that aims to protect the stability of the power system during an emergency, by dividing the grid into several partitions and isolating the elements that would cause cascading failures. This paper proposes a deep learning method to solve the problem of intentional islanding in an end-to-end manner. Two types of loss functions are examined for the graph partitioning task, and a loss function is added on the deep learning model, aiming to minimise the load-generation imbalance in the formed islands. In addition, the proposed solution incorporates a technique for merging the independent buses to their nearest neighbour in case there are isolated buses after the clusterisation, improving the final result in cases of large and complex systems. Several experiments demonstrate that the introduced deep learning method provides effective clustering results for intentional islanding, managing to keep the power imbalance low and creating stable islands. Finally, the proposed method is dynamic, relying on real-time system conditions to calculate the result.},
DOI = {10.3390/s21051650}
}

@ARTICLE{imagesegmentation,

  author={Grady, L. and Schwartz, E.L.},

  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 

  title={Isoperimetric graph partitioning for image segmentation}, 

  year={2006},

  volume={28},

  number={3},

  pages={469-475},

  doi={10.1109/TPAMI.2006.57}}

@article{gnnsurvey,
author = {Zhang, Ziwei and Cui, Peng and Zhu, Wenwu},
year = {2020},
month = {03},
pages = {1-1},
title = {Deep Learning on Graphs: A Survey},
volume = {PP},
journal = {IEEE Transactions on Knowledge and Data Engineering},
doi = {10.1109/TKDE.2020.2981333}
}

@article{gcnreview,
author = {Zhang, Si and Tong, Hanghang and Xu, Jiejun and Maciejewski, Ross},
year = {2019},
month = {11},
pages = {},
title = {Graph convolutional networks: a comprehensive review},
volume = {6},
journal = {Computational Social Networks},
doi = {10.1186/s40649-019-0069-y}
}

@article{gcn,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}

@article{nodefeatures,
  author    = {Chen Wang and
               Yingtong Dou and
               Min Chen and
               Jia Chen and
               Zhiwei Liu and
               Philip S. Yu},
  title     = {Deep Fraud Detection on Non-attributed Graph},
  journal   = {CoRR},
  volume    = {abs/2110.01171},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.01171},
  eprinttype = {arXiv},
  eprint    = {2110.01171},
  timestamp = {Fri, 08 Oct 2021 15:47:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-01171.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Book{appcombinatorial,
author = {Korte, Bernhard and Vygen, Jens},
title = {Combinatorial Optimization: Theory and Algorithms},
year = {2012},
isbn = {3642244874},
publisher = {Springer Publishing Company, Incorporated},
edition = {5th},
abstract = {This comprehensive textbook on combinatorial optimization places specialemphasis on theoretical results and algorithms with provably goodperformance, in contrast to heuristics. It is based on numerous courses on combinatorial optimization and specialized topics, mostly at graduate level. This book reviews the fundamentals, covers the classical topics (paths, flows, matching, matroids, NP-completeness, approximation algorithms) in detail, and proceeds to advanced and recent topics, some of which have not appeared in a textbook before. Throughout,it contains complete but concise proofs, and also provides numerousexercises and references. This fifth edition has again been updated, revised, and significantlyextended, with more than 60 new exercises and new material on varioustopics, including Cayley's formula, blocking flows, fasterb-matching separation, multidimensional knapsack, multicommoditymax-flow min-cut ratio, and sparsest cut. Thus, this book represents the state of the art of combinatorial optimization.}
}

@misc{brilliant, 
    title={Combinatorial optimization}, 
    url={https://brilliant.org/wiki/combinatorial-optimization/}, 
    howpublished = {\url{ https://brilliant.org/wiki/combinatorial-optimization/ }}, 
    journal={Brilliant Math and Science Wiki}, 
    author={Henry Maltby and Eli Ross }, 
    year = {2022} 
}

@phdthesis{fernandes, 
title={Algorithms and Models For Combinatorial Optimization Problems}, 
author={Fernandes, Albert Einstein Muritiba}, 
year={2010}
}

@book{schrijver-book,
  added-at = {2007-07-05T16:17:35.000+0200},
  author = {Schrijver, A.},
  biburl = {https://www.bibsonomy.org/bibtex/2496d0012f9b295acbef270a129061375/jleny},
  description = {bandit problems},
  interhash = {dfbeb3a87380195540f44a10e9995230},
  intrahash = {496d0012f9b295acbef270a129061375},
  keywords = {imported},
  publisher = {Springer},
  timestamp = {2007-07-05T16:17:37.000+0200},
  title = {Combinatorial Optimization - Polyhedra and Efficiency},
  year = {2003}
}

@Inbook{integeroptimization,
author="Hoffman, Karla L.
and Ralphs, Ted K.",
editor="Gass, Saul I.
and Fu, Michael C.",
title="Integer and Combinatorial Optimization",
bookTitle="Encyclopedia of Operations Research and Management Science",
year="2013",
publisher="Springer US",
address="Boston, MA",
pages="771--783",
isbn="978-1-4419-1153-7",
doi="10.1007/978-1-4419-1153-7_129",
url="https://doi.org/10.1007/978-1-4419-1153-7_129"
}

@article{branchbound,
author = {Morrison, David R. and Jacobson, Sheldon H. and Sauppe, Jason J. and Sewell, Edward C.},
title = {Branch-and-Bound Algorithms},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {19},
number = {C},
issn = {1572-5286},
url = {https://doi.org/10.1016/j.disopt.2016.01.005},
doi = {10.1016/j.disopt.2016.01.005},
abstract = {The branch-and-bound (B&B) algorithmic framework has been used successfully to find exact solutions for a wide array of optimization problems. B&B uses a tree search strategy to implicitly enumerate all possible solutions to a given problem, applying pruning rules to eliminate regions of the search space that cannot lead to a better solution. There are three algorithmic components in B&B that can be specified by the user to fine-tune the behavior of the algorithm. These components are the search strategy, the branching strategy, and the pruning rules. This survey presents a description of recent research advances in the design of B&B algorithms, particularly with regards to these three components. Moreover, three future research directions are provided in order to motivate further exploration in these areas.},
journal = {Discret. Optim.},
month = {feb},
pages = {79–102},
numpages = {24},
keywords = {Cyclic best first search, 90C10, 90C57, Discrete optimization, Survey, 90C27, Branch-and-bound, Integer programming, 90-02, Search strategies}
}

@book{heuristics,
author = {Pearl, Judea},
title = {Heuristics:  Intelligent Search Strategies for Computer Problem Solving},
year = {1984},
isbn = {0201055945},
publisher = {Addison-Wesley Longman Publishing Co., Inc.},
address = {USA}
}

@misc{branchimage,
  title = {Introduction into the Modeling and Optimization of Linear Systems},
  howpublished = {\url{https://hegyhati.github.io/IMOLS/}},
  note = {Accessed: 2010-09-30},
  author = {Máté Hegyháti},
  year = {2022}
}

@article{complexnetworks,
author = {Dirk Brockmann  and Dirk Helbing },
title = {The Hidden Geometry of Complex, Network-Driven Contagion Phenomena},
journal = {Science},
volume = {342},
number = {6164},
pages = {1337-1342},
year = {2013},
doi = {10.1126/science.1245200},
URL = {https://www.science.org/doi/abs/10.1126/science.1245200},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1245200},
abstract = {In combating the global spread of an emerging infectious disease, answers must be obtained to three crucial questions: Where did the disease emerge? Where will it go next? When will it arrive? Brockmann and Helbing (p. 1337; see the Perspective by McLean) analyzed disease spread via the “effective distance” rather than geographical distance, wherein two locations that are connected by a strong link are effectively close. The approach was successfully applied to predict disease arrival times or disease source using data from the the 2003 SARS viral epidemic, 2009 H1N1 influenza pandemic, and the 2011 foodborne enterohaemorrhagic Escherichia coli outbreak in Germany. A model based on effective rather than geographical distance can reveal the origin, timing, and likely spread of epidemics. [Also see Perspective by McLean] The global spread of epidemics, rumors, opinions, and innovations are complex, network-driven dynamic processes. The combined multiscale nature and intrinsic heterogeneity of the underlying networks make it difficult to develop an intuitive understanding of these processes, to distinguish relevant from peripheral factors, to predict their time course, and to locate their origin. However, we show that complex spatiotemporal patterns can be reduced to surprisingly simple, homogeneous wave propagation patterns, if conventional geographic distance is replaced by a probabilistically motivated effective distance. In the context of global, air-traffic–mediated epidemics, we show that effective distance reliably predicts disease arrival times. Even if epidemiological parameters are unknown, the method can still deliver relative arrival times. The approach can also identify the spatial origin of spreading processes and successfully be applied to data of the worldwide 2009 H1N1 influenza pandemic and 2003 SARS epidemic.}
}

@article{bettersuited,
  title={Fuzzy lattice reasoning (FLR) type neural computation for weighted graph partitioning},
  author={Vassilis G. Kaburlasos and Lefteris Moussiades and Athena Vakali},
  journal={Neurocomputing},
  year={2009},
  volume={72},
  pages={2121-2133}
}

@inproceedings{sparcification,
author = {Spielman, Daniel A. and Teng, Shang-Hua},
title = {Nearly-Linear Time Algorithms for Graph Partitioning, Graph Sparsification, and Solving Linear Systems},
year = {2004},
isbn = {1581138520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1007352.1007372},
doi = {10.1145/1007352.1007372},
abstract = {We present algorithms for solving symmetric, diagonally-dominant linear systems to accuracy ε in time linear in their number of non-zeros and log (κf (A) ε), where κf (A) is the condition number of the matrix defining the linear system. Our algorithm applies the preconditioned Chebyshev iteration with preconditioners designed using nearly-linear time algorithms for graph sparsification and graph partitioning.},
booktitle = {Proceedings of the Thirty-Sixth Annual ACM Symposium on Theory of Computing},
pages = {81–90},
numpages = {10},
keywords = {preconditioners, graph partitioning, graph sparsification},
location = {Chicago, IL, USA},
series = {STOC '04}
}

@article{nesteddissection,
  title={Graph Partitioning and Sparse Matrix Ordering using Reinforcement Learning},
  author={Alice Gatti and Zhixiong Hu and Pieter Ghysels and Esmond G. Ng and Tess E. Smidt},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.03546}
}

@ARTICLE{matrixordering,
  author={Gupta, A.},
  journal={IBM Journal of Research and Development}, 
  title={Fast and effective algorithms for graph partitioning and sparse-matrix ordering}, 
  year={1997},
  volume={41},
  number={1.2},
  pages={171-183},
  doi={10.1147/rd.411.0171}
}
