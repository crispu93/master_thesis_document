@article{hamilton,
    author={Hamilton, William L.},
    title={Graph Representation Learning},
    journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
    volume={14},
    number={3},
    pages={1-159},
    publisher={Morgan and Claypool}
}

@inbook{gatti,
    author = {Alice Gatti and Zhixiong Hu and Tess Smidt and Esmond G. Ng and Pieter Ghysels},
    title = {Deep Learning and Spectral Embedding for Graph Partitioning},
    booktitle = {Proceedings of the 2022 SIAM Conference on Parallel Processing for Scientific Computing (PP)},
    chapter = {},
    pages = {25-36},
    doi = {10.1137/1.9781611977141.3},
    URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977141.3},
    eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611977141.3},
        abstract = { Abstract We present a graph bisection and partitioning algorithm based on graph neural networks. For each node in the graph, the network outputs probabilities for each of the partitions. The graph neural network consists of two modules: an embedding phase and a partitioning phase. The embedding phase is trained first by minimizing a loss function inspired by spectral graph theory. The partitioning module is trained through a loss function that corresponds to the expected value of the normalized cut. Both parts of the neural network rely on SAGE convolutional layers and graph coarsening using heavy edge matching. The multilevel structure of the neural network is inspired by the multigrid algorithm. Our approach generalizes very well to bigger graphs and has partition quality comparable to METIS, Scotch and spectral partitioning, with shorter runtime compared to METIS and spectral partitioning. }
}

@article{gap,
  title={GAP: Generalizable Approximate Graph Partitioning Framework},
  author={Azade Nazi and Will Hang and Anna Goldie and Sujith Ravi and Azalia Mirhoseini},
  journal={ArXiv},
  year={2019},
  volume={abs/1903.00614}
}

@article{metis,
  title={A Fast and Highly Quality Multilevel Scheme for Partitioning Irregular Graphs},
  author={George Karypis and Vipin Kumar},
  journal={SIAM Journal on Scientific Computing},
  year={1999},
  volume={20},
  number={1},
  pages={359—392}
}

@inproceedings{graphsage,
    author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
    title = {Inductive Representation Learning on Large Graphs},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {1025–1035},
    numpages = {11},
    location = {Long Beach, California, USA},
    series = {NIPS'17}
    }

@article{archive,
author = {Soper, Alan and Walshaw, Chris and Cross, Mark},
year = {2004},
month = {06},
pages = {225-241},
title = {A Combined Evolutionary Search and Multilevel Optimisation Approach to Graph-Partitioning},
volume = {29},
journal = {Journal of Global Optimization},
doi = {10.1023/B:JOGO.0000042115.44455.f3}
}

@inbook{gap2,
author = {Alice Gatti and Zhixiong Hu and Tess Smidt and Esmond G. Ng and Pieter Ghysels},
title = {Deep Learning and Spectral Embedding for Graph Partitioning},
booktitle = {Proceedings of the 2022 SIAM Conference on Parallel Processing for Scientific Computing (PP)},
chapter = {},
pages = {25-36},
doi = {10.1137/1.9781611977141.3},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611977141.3},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611977141.3},
    abstract = { Abstract We present a graph bisection and partitioning algorithm based on graph neural networks. For each node in the graph, the network outputs probabilities for each of the partitions. The graph neural network consists of two modules: an embedding phase and a partitioning phase. The embedding phase is trained first by minimizing a loss function inspired by spectral graph theory. The partitioning module is trained through a loss function that corresponds to the expected value of the normalized cut. Both parts of the neural network rely on SAGE convolutional layers and graph coarsening using heavy edge matching. The multilevel structure of the neural network is inspired by the multigrid algorithm. Our approach generalizes very well to bigger graphs and has partition quality comparable to METIS, Scotch and spectral partitioning, with shorter runtime compared to METIS and spectral partitioning. }
}

@inproceedings{xavier,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Xavier Glorot and Yoshua Bengio},
  booktitle={AISTATS},
  year={2010}
}

@inproceedings{gap1,
  title={A DEEP LEARNING FRAMEWORK FOR GRAPH PARTITIONING},
  author={Anna Goldie and Sujith Ravi and Azalia Mirhoseini},
  year={2019}
}

@inproceedings{deepwalk,
author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
title = {DeepWalk: Online Learning of Social Representations},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623732},
doi = {10.1145/2623330.2623732},
abstract = {We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data.DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {701–710},
numpages = {10},
keywords = {latent representations, deep learning, social networks, network classification, learning with partial labels, online learning},
location = {New York, New York, USA},
series = {KDD '14}
}

@inbook{deepwalk_hyper,
author = {Yunfang, Chen and Wang, Li and Qi, Dehao and Zhang, Wei},
year = {2020},
month = {08},
pages = {568-583},
title = {Community Detection Based on DeepWalk in Large Scale Networks},
isbn = {978-981-15-7529-7},
doi = {10.1007/978-981-15-7530-3_43}
}
  
@inproceedings{karateclub,
               title = {{Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs}},
               author = {Benedek Rozemberczki and Oliver Kiss and Rik Sarkar},
               year = {2020},
               pages = {3125–3132},
               booktitle = {Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM '20)},
               organization = {ACM},
}

